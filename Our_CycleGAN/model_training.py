import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from torch.optim.lr_scheduler import LambdaLR 

from PIL import Image
import os
import pandas as pd
import numpy as np
from tqdm import tqdm
import random
import time
import json
import pickle
from CycleGAN_arch import CycleGAN

class CustomDataset(Dataset):
    def __init__(self, dataframe, transform=None):
        self.dataframe = dataframe
        self.transform = transform

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        A_path = self.dataframe.iloc[idx, 1]
        B_path = self.dataframe.iloc[idx, 3]

        A_image = Image.open(A_path).convert("RGB")
        B_image = Image.open(B_path).convert("RGB")

        if self.transform:
            A_image = self.transform(A_image)
            B_image = self.transform(B_image)

        return A_image, B_image
    
class ImageBuffer():
    """
    A buffer to store the last 50 generated images.
    Used to train the Discriminator with a mix of current and older fake samples
    for stability, as suggested by the original CycleGAN paper.
    """
    def __init__(self, max_size=50):
        self.max_size = max_size
        self.data = []

    def push_and_pop(self, images):
        to_return = []
        for image in images:
            image = torch.unsqueeze(image.data, 0)
            if len(self.data) < self.max_size:
                self.data.append(image)
                to_return.append(image)
            else:
                if random.uniform(0, 1) > 0.5:
                    # Replace an old image with the new one 50% of the time
                    idx = random.randint(0, self.max_size - 1)
                    to_return.append(self.data[idx].clone())
                    self.data[idx] = image
                else:
                    # Use the new image 50% of the time without replacement
                    to_return.append(image)
        return torch.cat(to_return, 0)

class Trainer():
    
    def __init__(
                self, device, model_name, model,
                lr=0.0002, 
                beta1=0.5, 
                beta2=0.999, 
                batch_size=1, 
                n_epochs=200, 
                n_epochs_decay_start=100, 
                lambda_cycle=10.0, 
                lambda_identity=0.5, 
                history_step=10, 
                init_weights=True
                ):
        
        # device
        self.device = device
        
        # Hyperparameters
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.batch_size = batch_size
        self.n_epochs = n_epochs
        self.n_epochs_decay_start = n_epochs_decay_start
        self.lambda_cycle = lambda_cycle
        if not lambda_identity:
            lambda_identity = 0.5 * self.lambda_cycle
        self.lambda_identity = lambda_identity
        
        # critirion
        self.criterion_GAN = nn.MSELoss()
        self.criterion_cycle = nn.L1Loss()
        self.criterion_identity = nn.L1Loss()
        
        # model
        self.model = model(device)
        self.model_net = [self.model.G_A2B, self.model.G_B2A, self.model.D_A, self.model.D_B]
        if init_weights:
            for net in self.model_net:
                self.init_weights(net)
        
        # optimizer
        self.optimizer_G = None
        self.optimizer_D_A = None
        self.optimizer_D_B = None
        self.optimizer_init()
        
        # training variables
        self.start_epoch = 0
        self.history_step = history_step  
        self.best_G_loss = float('inf')
        
        # optimizer scheduler
        self.scheduler_G = None
        self.scheduler_D_A = None
        self.scheduler_D_B = None
        self.scheduler_init()   
        
        # Initialize buffers for both domains
        self.fake_A_buffer = ImageBuffer() # Buffer for images generated by G_B2A (Normal -> Low)
        self.fake_B_buffer = ImageBuffer() # Buffer for images generated by G_A2B (Low -> Normal)

        # checkpoint path
        self.checkpoint_dir = "./Our_CycleGAN/checkpoints"
        if not os.path.exists(self.checkpoint_dir):
            os.makedirs(self.checkpoint_dir)
        self.checkpoint_filename = "cyclegan_checkpoint.pth" 
        self.checkpoint_filepath = os.path.join(self.checkpoint_dir, self.checkpoint_filename)
        
        # history path
        self.history_dir = "./Our_CycleGAN/training_history"
        if not os.path.exists(self.history_dir):
            os.makedirs(self.history_dir)
            
        # model path
        self.model_dir = "./Our_CycleGAN/models"
        if not os.path.exists(self.model_dir):
            os.makedirs(self.model_dir)
        self.model_name = model_name
        
        # model filename
        self.model_filename = None
        
    @staticmethod
    def get_cyclegan_transforms():
        """
        Defines the standard transformations for CycleGAN training data.
        These align with the guide's recommendation for aggressive augmentation
        and normalization to [-1, 1].
        """
        # Parameters for resizing and cropping
        resize_to = 286
        crop_size = 256
        
        # Normalization for GANs: scales [0, 1] to [-1, 1]
        normalize = transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))

        return transforms.Compose([
            transforms.Resize(resize_to, Image.BICUBIC),
            transforms.RandomCrop(crop_size),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            normalize
        ])
        
    @staticmethod
    def init_weights(net, init_type='normal', init_gain=0.02):
        """
        Initialize network weights with N(0, 0.02) Gaussian distribution.
        
        Args:
            net (nn.Module): The network to initialize.
            init_type (str): 'normal' for Gaussian.
            init_gain (float): Standard deviation for Gaussian (0.02).
        """
        def init_func(m):
            classname = m.__class__.__name__
            if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):
                if init_type == 'normal':
                    nn.init.normal_(m.weight.data, 0.0, init_gain)
                
                if hasattr(m, 'bias') and m.bias is not None:
                    nn.init.constant_(m.bias.data, 0.0)
            elif classname.find('BatchNorm2d') != -1:
                nn.init.normal_(m.weight.data, 1.0, init_gain)
                nn.init.constant_(m.bias.data, 0.0)
                
        

        print(f'Initializing network {net.__class__.__name__} with {init_type} (gain={init_gain})')
        net.apply(init_func)
        
    def optimizer_init(self):
        """
        Initializes the optimizers for the CycleGAN model.
        """
        self.optimizer_D_A = optim.Adam(self.model.D_A.parameters(), lr=self.lr, betas=(self.beta1, self.beta2))
        self.optimizer_D_B = optim.Adam(self.model.D_B.parameters(), lr=self.lr, betas=(self.beta1, self.beta2))
        self.optimizer_G = optim.Adam(list(self.model.G_A2B.parameters()) + list(self.model.G_B2A.parameters()), lr=self.lr, betas=(self.beta1, self.beta2))
        
    def scheduler_init(self):
        """
        Initializes the learning rate schedulers for the CycleGAN model.
        """
        
        def lambda_rule(epoch):
            """
            Linear decay schedule:
            Factor 1.0 (constant LR) for first N_EPOCHS_DECAY_START (100) epochs.
            Linear decay from factor 1.0 to 0.0 over the next N_EPOCHS_TOTAL - N_EPOCHS_DECAY_START (100) epochs.
            """
            # Adjust epoch for resume, using the global start_epoch
            current_relative_epoch = epoch + self.start_epoch
            
            if current_relative_epoch < self.n_epochs_decay_start:
                return 1.0
            else:
                # Calculate decay factor
                decay_epochs =  self.n_epochs - self.n_epochs_decay_start
                # Epoch index into decay phase (starts at 0 when current_relative_epoch = n_epochs_decay_start)
                epoch_in_decay = current_relative_epoch - self.n_epochs_decay_start
                return 1.0 - max(0, epoch_in_decay) / decay_epochs
        
        self.scheduler_G = LambdaLR(self.optimizer_G, lr_lambda=lambda_rule)
        self.scheduler_D_A = LambdaLR(self.optimizer_D_A, lr_lambda=lambda_rule)
        self.scheduler_D_B = LambdaLR(self.optimizer_D_B, lr_lambda=lambda_rule)
        
    def save_best_model(self):
        state = {
            'G_A2B_state_dict': self.model.G_A2B.state_dict(),
            'G_B2A_state_dict': self.model.G_B2A.state_dict(),
            'D_A_state_dict': self.model.D_A.state_dict(),
            'D_B_state_dict': self.model.D_B.state_dict(),
        }
        file_path = os.path.join(self.model_dir, self.model_name)
        torch.save(state, file_path)
        print(f"Model saved to {file_path}")
        
    def save_checkpoint(self, epoch, best_G_loss, filename = ""):
        """Saves the state of all models and optimizers."""
        if not filename:
            filepath = self.checkpoint_filepath
        else:
            filepath = os.path.join(self.checkpoint_dir, filename)
        
        state = {
            'epoch': epoch,
            'best_G_loss': best_G_loss,
            'G_A2B_state_dict': self.model.G_A2B.state_dict(),
            'G_B2A_state_dict': self.model.G_B2A.state_dict(),
            'D_A_state_dict': self.model.D_A.state_dict(),
            'D_B_state_dict': self.model.D_B.state_dict(),
            'optimizer_G_state_dict': self.optimizer_G.state_dict(),
            'optimizer_D_A_state_dict': self.optimizer_D_A.state_dict(),
            'optimizer_D_B_state_dict': self.optimizer_D_B.state_dict(),
        }
        torch.save(state, filepath)
        print(f"Checkpoint saved to {filepath} (Epoch {epoch+1}, Loss: {best_G_loss:.4f})")
        
    def load_checkpoint(self):
        """Loads state from a checkpoint file if it exists."""
        if os.path.exists(self.checkpoint_filepath):
            
            try:
                print(f"Loading checkpoint from {self.checkpoint_filepath}...")
                checkpoint = torch.load(self.checkpoint_filepath, map_location=self.device, weights_only=False)
                
                # Load states
                self.model.G_A2B.load_state_dict(checkpoint['G_A2B_state_dict'])
                self.model.G_B2A.load_state_dict(checkpoint['G_B2A_state_dict'])
                self.model.D_A.load_state_dict(checkpoint['D_A_state_dict'])
                self.model.D_B.load_state_dict(checkpoint['D_B_state_dict'])
                
                self.optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])
                self.optimizer_D_A.load_state_dict(checkpoint['optimizer_D_A_state_dict'])
                self.optimizer_D_B.load_state_dict(checkpoint['optimizer_D_B_state_dict'])
                
                # Resume training parameters
                self.start_epoch = checkpoint['epoch'] + 1
                self.best_G_loss = checkpoint['best_G_loss']
                
                print(f"Checkpoint loaded successfully. Resuming from Epoch {self.start_epoch} with best G Loss: {self.best_G_loss:.4f}")
                return True
            except Exception as e:
                print(f"Error loading checkpoint: {e}. Starting fresh.")
                return False
        else:
            print(f"No checkpoint found at {self.checkpoint_filepath}. Starting fresh.")
            return False
        
    def train_loop(self, df_train):
        total_start_time = time.time()  # Track total training time
        train_history = [] # To store training history
        
        start_epoch_history = self.start_epoch
        end_epoch_history = start_epoch_history + self.history_step
        
        for epoch in range(self.start_epoch, self.n_epochs):
            train_loader = self.create_train_loader(df_train)
            
            progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{self.n_epochs}")
            avg_metrics = {k: [] for k in ['G_Total', 'G_A2B', 'G_B2A', 'Cycle', 'Id', 'D_A', 'D_B']}
            
            if epoch+1 == end_epoch_history:
                file_name = f"Training_log_history_ep{start_epoch_history}_to_{end_epoch_history-1}.json"
                file_path = os.path.join(self.history_dir, file_name)
                with open(file_path, "w") as f:
                    json.dump(train_history, f)
                start_epoch_history = end_epoch_history
                end_epoch_history = start_epoch_history + self.history_step
                train_history = []
                
            for i, (real_A, real_B) in enumerate(progress_bar):
                metrics = self.train_step(real_A, real_B)
                
                # --- LOG LOSS FOR HISTORY ---
                log_entry = {
                    'epoch': epoch,
                    'iteration': i,
                    'metrics': {k: float(v) for k, v in metrics.items()} # Convert to float for JSON safety
                }
                
                train_history.append(log_entry)
                
                # Log metrics for TQDM
                for key, value in metrics.items():
                    avg_metrics[key].append(value)

                log_dict = {k: np.mean(v) for k, v in avg_metrics.items()}
                progress_bar.set_postfix(log_dict)
                
            # Calculate average Generator & Discriminator Loss for the epoch
            current_avg_G_loss = np.mean(avg_metrics['G_Total'])
            current_avg_D_A_loss = np.mean(avg_metrics['D_A'])
            current_avg_D_B_loss = np.mean(avg_metrics['D_B'])

            print(f"\n--- Epoch {epoch+1} Complete ---")
            print(f"Avg Loss G: {current_avg_G_loss:.4f} | Avg Loss D_A: {current_avg_D_A_loss:.4f} | Avg Loss D_B: {current_avg_D_B_loss:.4f}")
                
            # Save checkpoint if the average G loss is the best we've seen
            if current_avg_G_loss < self.best_G_loss:
                self.best_G_loss = current_avg_G_loss
                self.save_checkpoint(epoch, self.best_G_loss)
            
            # Optionally, save a checkpoint every N epochs regardless of performance
            self.save_checkpoint(epoch, current_avg_G_loss, filename="Best_" + self.checkpoint_filename)
            
            # --- Apply LR Decay Schedulers ---
            self.scheduler_G.step()
            self.scheduler_D_A.step()
            self.scheduler_D_B.step()
        
        total_end_time = time.time()
        total_duration = total_end_time - total_start_time
        print(f"\nâœ… Training Complete! Total Time: {total_duration/60:.2f} min ({total_duration/3600:.2f} hr)")
        
        self.save_best_model()
    
    def train_step(self, real_A, real_B):
        """
        Performs a single training step, updating both Generators and Discriminators.
        A = Low Light, B = Normal Light
        """
        # Move data to device
        real_A = real_A.to(self.device)
        real_B = real_B.to(self.device)

        # Define target tensors for GAN loss (LSGAN uses 1.0 for real, 0.0 for fake)
        # image at D model in last channel is 1x30x30
        real_target = torch.full((real_A.size(0), 1, 30, 30), 1.0, device=self.device) # image at D model in last channel is 30x30
        fake_target = torch.full((real_A.size(0), 1, 30, 30), 0.0, device=self.device)
        
        # ---------------------------------------------------
        # 1. Update Generators (G_A2B and G_B2A)
        # ---------------------------------------------------
        loss_G_total, loss_G_A2B, loss_G_B2A, loss_cycle, loss_identity, fake_A, fake_B = self.gan_training_step(real_A, real_B, real_target, fake_target)
        
        # ---------------------------------------------------
        # 2. Update Discriminators (D_A and D_B)
        # ---------------------------------------------------
        loss_D_A, loss_D_B = self.discriminator_training_step(real_A, real_B, real_target, fake_target, fake_A, fake_B)

        # --- Return Loss Metrics ---
        metrics = {
            'G_Total': loss_G_total.item(),
            'G_A2B': loss_G_A2B.item(),
            'G_B2A': loss_G_B2A.item(),
            'Cycle': loss_cycle.item(),
            'Id': loss_identity.item(),
            'D_A': loss_D_A.item(),
            'D_B': loss_D_B.item(),
        }
        return metrics
    
    def gan_training_step(self, real_A, real_B, real_target, fake_target):
        # Set generator models to training mode
        self.model.G_A2B.train()
        self.model.G_B2A.train()
        self.optimizer_G.zero_grad()

        # A. Identity Loss (Optional, but standard for CycleGAN)
        # G_A2B should output B when given B (Normal -> Normal)
        id_B = self.model.G_A2B(real_B)
        loss_id_B = self.criterion_identity(id_B, real_B)

        # G_B2A should output A when given A (Low -> Low)
        id_A = self.model.G_B2A(real_A)
        loss_id_A = self.criterion_identity(id_A, real_A)
        
        loss_identity = self.identity_objective(loss_id_A, loss_id_B)

        # B. GAN Loss (Generators want to fool the discriminators)
        # G_A2B(A) should look like B (Normal)
        fake_B = self.model.G_A2B(real_A)
        pred_fake_B = self.model.D_B(fake_B)
        loss_G_A2B = self.criterion_GAN(pred_fake_B, real_target)

        # G_B2A(B) should look like A (Low)
        fake_A = self.model.G_B2A(real_B)
        pred_fake_A = self.model.D_A(fake_A)
        loss_G_B2A = self.criterion_GAN(pred_fake_A, real_target)
        
        # C. Cycle Consistency Loss (Recycle images back)
        # Cycle A: A -> B -> A (real_A should be close to G_B2A(G_A2B(real_A)))
        reconstructed_A = self.model.G_B2A(fake_B)
        loss_cycle_A = self.criterion_cycle(reconstructed_A, real_A)

        # Cycle B: B -> A -> B (real_B should be close to G_A2B(G_B2A(real_B)))
        reconstructed_B = self.model.G_A2B(fake_A)
        loss_cycle_B = self.criterion_cycle(reconstructed_B, real_B)
        
        loss_cycle = self.cycle_objective(loss_cycle_A, loss_cycle_B)

        # D. Total Generator Loss and Update
        loss_G_total = self.gan_objective(loss_G_A2B, loss_G_B2A, loss_cycle, loss_identity)
            
        loss_G_total.backward()
        self.optimizer_G.step()
        
        return loss_G_total, loss_G_A2B, loss_G_B2A, loss_cycle, loss_identity, fake_A, fake_B
    
    def discriminator_training_step(self, real_A, real_B, real_target, fake_target, fake_A, fake_B):
        # Set discriminator models to training mode
        self.model.D_A.train()
        self.model.D_B.train()
        self.optimizer_D_A.zero_grad()
        self.optimizer_D_B.zero_grad()

        # E. Update D_B (Discriminator for Normal Light B)
        # Real loss D_B
        pred_real_B = self.model.D_B(real_B)
        loss_D_B_real = self.criterion_GAN(pred_real_B, real_target)
        
        # Fake loss D_B (Use buffered fake image)
        fake_B_buffered = self.fake_B_buffer.push_and_pop(fake_B)
        pred_fake_B_buff = self.model.D_B(fake_B_buffered.detach()) # Detach is crucial here!
        loss_D_B_fake = self.criterion_GAN(pred_fake_B_buff, fake_target)
        
        loss_D_B = self.discriminator_objective(loss_D_B_real, loss_D_B_fake)
        loss_D_B.backward()
        self.optimizer_D_B.step()

        # F. Update D_A (Discriminator for Low Light A)
        # Real loss D_A
        pred_real_A = self.model.D_A(real_A)
        loss_D_A_real = self.criterion_GAN(pred_real_A, real_target)

        # Fake loss D_A (Use buffered fake image)
        fake_A_buffered = self.fake_A_buffer.push_and_pop(fake_A)
        pred_fake_A_buff = self.model.D_A(fake_A_buffered.detach()) # Detach is crucial here!
        loss_D_A_fake = self.criterion_GAN(pred_fake_A_buff, fake_target)

        loss_D_A = self.discriminator_objective(loss_D_A_real, loss_D_A_fake)
        loss_D_A.backward()
        self.optimizer_D_A.step()
        
        return loss_D_A, loss_D_B
    
    def gan_objective(self, loss_G_A2B, loss_G_B2A, loss_cycle, loss_identity):
        loss_gan = loss_G_A2B + loss_G_B2A
        total_loss = (loss_gan) + (loss_cycle * self.lambda_cycle) + (loss_identity * self.lambda_identity)
        return total_loss
    
    def cycle_objective(self, loss_cycle_A, loss_cycle_B):  
        return loss_cycle_A + loss_cycle_B
    
    def identity_objective(self, loss_id_A, loss_id_B):
        return loss_id_A + loss_id_B
    
    def discriminator_objective(self, loss_D_real, loss_D_fake):
        return (loss_D_real + loss_D_fake) * 0.5    
        
    def create_train_loader(self, df_train):
        # --- CYCLEGAN "UNPAIRED" SHUFFLING TRICK (Applied per epoch) ---
        # 1. Shuffle the paths in both columns independently to break the pairing
        df_train.loc[:,"low_light_path"] = df_train["low_light_path"].sample(frac=1).values
        df_train.loc[:,"normal_light_path"] = df_train["normal_light_path"].sample(frac=1).values
        
        # 2. Re-instantiate the Dataset and DataLoader with the new, unpaired DataFrame
        new_train_dataset = CustomDataset(df_train, transform=self.get_cyclegan_transforms())
        # shuffle=False is typically used here because the DataFrame columns are already shuffled.
        # However, keeping shuffle=True here won't hurt, but we set it to False for clarity.
        train_loader = DataLoader(new_train_dataset, batch_size=1, shuffle=False)
        # ----------------------------------------------------------------------
        
        return train_loader
        
    def start_train(self, df_train):
        # --- MODEL TRAINING ---
        print(f"\nStarting CycleGAN Training for {self.n_epochs - self.start_epoch} epochs (Total {self.n_epochs})...")
        self.train_loop(df_train)
        






